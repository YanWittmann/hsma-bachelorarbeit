\chapter{Evaluation}\label{ch:evaluation}

Das in dieser Arbeit entwickelte Korrelationssystem soll in diesem Kapitel anhand von Qualitätsmetriken untersucht, die Ergebnisse dieser Analyse vorgestellt und diskutiert werden.


\section{Prüfen der Qualitätsmetriken}

Anforderung \hyperref[subsec:req-graph-inner-consistency]{A-19} definiert Qualitäts‑ und Integritätsbedingungen auf, die das neue Korrelationssystem erfüllen sollte.
Im Folgenden werden die einzelnen Metriken beschrieben und es wird erläutert, wie der Graph diesen Anforderungen gerecht wird oder wo noch Lücken bestehen.

\paragraph{Maximal eine Knotenidentifikation pro Repräsentation}
Auf dieses Ziel wurde in der Modellierung des Graphens in \autoref{subsubsec:model-matching} eingegangen, und es wurde durch mehrere Maßnahmen verfolgt.
Indem der Graph bei der Identifikation in einer Vererbungshierarchie immer nur den spezifischsten Knoten zur Identifikation auswählt und alle darüber und darunterliegenden verwirft, ist dieses Szenario abgedeckt.
Indem die Notwendigkeit, mehrere versionsspezifische Repräsentationsknoten zur Versionserkennung zu definieren, durch die zentralen Transformationen in den dazwischenliegenden Produktknoten genommen wurde, muss hier nur noch ein Knoten pro Repräsentation definiert werden.
Allgemein ist es nun ebenfalls nicht mehr nötig, eine Repräsentation mehrfach zu modellieren, da ein einziger Knoten auf beliebig viele weitere Knoten mit einer Kante seine Beziehung dazu modellieren kann.
Diese Überlegungen stellen jedoch nur Muster bereit, wie eine Produktkonstellation modelliert werden kann und über manuelle Anpassungen am Graphen kann es dazu kommen, dass Situationen erzeugt werden, in denen die eindeutige Identifikation nicht mehr gegeben ist.
Dazu muss das Testframework des Graphens einen Datensatz an bereits manuell geprüften Inventaren führen und auf diese Situation prüfen, um sicherzustellen, dass weitere Modifikationen diese Integrität nicht gefährden.

\paragraph{Nur ein einziger positiver Produktknoten pro Repräsentation}
Als Erweiterung zu der vorhergegangenen Metrik soll eine Repräsentation nur einen einzigen Produktknoten über eine \enquote{is}-Beziehung erreichen können.
Dies stellt das Konzept sicher, dass jede Produktmodellierung in sich konsistent ist.
Dieser Fall muss ebenfalls durch das Testframework recht einfach ohne Datensatz nur auf dem Graphen geprüft werden.

\paragraph{Keine losen Knoten im Graphen}
Um sicherzustellen, dass mit jeder Identifikation einer Repräsentation im Graphen ein Informationsgewinn stattfindet, muss auch für jeden Knoten über mindestens eine Kante ein weiterer Knoten erreicht werden können.
Diese Qualitätsmetrik lässt sich ebenfalls einfach überprüfen.
jedoch stellt sie einige ungelöste Herausforderungen auf konzeptioneller Ebene.
Wenn ein automatischer Contributor auf dem Graphen z.\ B.\ alle Knoten für die \acrshort{cpe}-Datenquelle mit ihren Metadaten erzeugt, dann kann nur basierend auf einer \acrshort{cpe} in den meisten Fällen nicht entschieden werden, welche weiteren Repräsentationen es dazu geben könnte, oder welchem abstrakten Produkt diese \acrshort{cpe} zugehört.
Eine Limitation dieser Qualitätsmetrik ausschließlich auf Knoten, die vom manuellen Modifikationsformat berührt wurden, macht diese Anforderung in der Praxis anwendbar und nützlich, indem von den automatisch erzeugten, mit Metadaten gefüllten Knoten profitiert werden kann, und dennoch sichergestellt wird, dass bei der manuellen Modifikation keine Knoten vergessen werden und mindestens ein Produktknoten zu einer Repräsentation modelliert werden muss.
Die Unterscheidung der Knoten kann über das \enquote{\texttt{contributors}}-Attribut geschehen, in dem alle auf einem Knoten beitragenden Schritte eingetragen werden.

\paragraph{Metadaten auf jedem Knoten und jeder Kante}
Um sicherzustellen, dass jede manuelle Entscheidung nachvollzogen werden kann, muss auf jedem Knoten und jeder Kante das Metadaten-Attribut gefüllt sein.
Hier präsentiert sich jedoch dieselbe Herausforderung mit den automatisch erzeugten Knoten und Kanten wie in der vorherigen Metrik, da diese Information nicht immer automatisch aus den Datenquellen abgeleitet werden kann.
Auch hier muss also eine Filterung der Prüfung auf diejenigen Datenelemente im Testframework passieren, die von dem manuellen Schritt berührt wurden.

\paragraph{Ein sich selbst pflegender Datensatz}
Wie in der ersten Metrik in diesem Kapitel bereits aufgeführt, muss das Testframework in der Lage sein, einen Bestand an Inventaren zu pflegen, für das es als Erwartungshaltung den jeweiligen Zustand der letzten Prüfung festhält und bei Abweichungen in der nächsten Iteration den Nutzer warnt.
Da dieses nicht Teil der bisherigen Implementierung ist, kann hier keine weitere Erklärung folgen.

\paragraph{Zirkuläre Vererbungshierarchien}
Im Testframework muss ebenfalls auf diese geprüft werden, um sicherzustellen, dass diese Situation nicht passieren kann.


\section{Performanceanalyse}\label{sec:evaluation-performanceanalyse}

In einem weiteren Durchlauf mit einem künstlich erzeugten Datensatz derselben Größe wie in \autoref{subsec:req-correlation-format-performance} wurde dasselbe Testinventar mit unterschiedlichen Größen auf die Laufzeit untersucht.
Bei der Betrachtung der Ergebnisse in \autoref{tab:new-correlation-performance} ist es wichtig zu betonen, dass auf der Implementierung des neuen Korrelationssystems neben dem Caching der Knotenpunkte noch keine weiteren Performance-Optimierungen angewendet wurden und dies die ersten Messungsergebnisse sind.
Der Vergleich der beiden Laufzeiten ist in \autoref{fig:correlation-performance-comparison} als Diagramm dargestellt.

Da der Graph im neuen Korrelationssystem aus der Datenbank nur bei Bedarf ausgelesen wird, ergibt es Sinn, dass es keine zusätzliche Startup-Zeit gibt, wenn keine Datenmengen angefordert werden.
Wie auch bei dem alten Korrelationssystem ist ein linearer Zuwachs zu erkennen, da für jedes zusätzliche Artefakt ähnlich viele Berechnungen vorgenommen werden müssen.
Auf eine realistische Anzahl an Artefakten betrachtet ist das neue System also bisher circa 1.8-Mal langsamer als das alte.

\begin{table}[h!]
    \centering
    \begin{tabular}{l r r r}
        \toprule
        \textbf{Artefakte} & \textbf{Ø [s]} & \textbf{Min. [s]} & \textbf{Max. [s]} \\
        \midrule
        0                  & 0,000          & 0,000             & 0,000             \\
        1                  & 0,034          & 0,001             & 0,134             \\
        500                & 2,303          & 2,078             & 2,950             \\
        1000               & 4,249          & 4,022             & 4,790             \\
        2000               & 8,582          & 8,109             & 9,658             \\
        3000               & 12,639         & 12,156            & 13,730            \\
        4000               & 16,702         & 16,113            & 17,446            \\
        5000               & 20,693         & 20,179            & 21,175            \\
        6000               & 24,601         & 24,093            & 25,503            \\
        \bottomrule
    \end{tabular}
    \caption{Laufzeiten des neuen Korrelationsformats bei unterschiedlichen Artefakt-Anzahlen.}
    \label{tab:new-correlation-performance}
\end{table}

\begin{figure}[htbp]
    \centering
    \makebox[\textwidth]{\includesvg[width=1\textwidth, inkscapelatex=false]{bilder/laufzeitvergleich}}
    \caption{Vergleich der Laufzeiten: Altes vs. neues Korrelationssystem}
    \label{fig:correlation-performance-comparison}
\end{figure}


\section{Diskussion}

In dieser Diskussion wird eine kritische Würdigung der Arbeit vorgenommen, in der zunächst aufgeführt wird, was die erreichten Ziele und Gewinne sind, und was noch Verbesserungswürdig ist.
Auf diesem Verbesserungspotential wird im Ausblick in \autoref{sec:schluss-ausblick} aufgebaut, um die nächsten Schritte zu definieren.

\subsection{Was ist gut gelaufen?}\label{subsec:discussion-positive}

Das neue Korrelationssystem wurde erfolgreich in einem funktionalen Prototyp konzipiert und implementiert und erfüllt die zentralen Anforderungen.
Alle Referenzfälle konnten in dem neuen System modelliert und getestet werden.
Da die Referenzfälle so gewählt wurden, dass sie die grundlegenden Fälle und aus verschiedenen Bereichen die kompliziertesten Fälle abdecken, bedeutet dies implizit auch, dass der große Großteil der verbleibenden Fälle abgebildet werden kann.

Der Punkt, an dem eine der größten Verbesserungen gemessen werden konnte, ist die Neu-Konzipierung der Artefakt-Selektion.
Die neuen Selektoren erlauben durch eine typbasierte Auswertung anstelle einer Kombination von Attributen eine wesentlich zuverlässigere Identifikation, die für zukünftige Anpassungen an der Erkennung offen ist.
Die Einführung typspezifischer Attribute, die in den Selektoren als first-class-Attribute verfügbar gemacht werden und die Möglichkeit, in einem Selektor mehrere Werte für ein Attribut prüfen zu können unterstützt die Zuverlässigkeit und reduziert die Redundanz weiter.
Durch die Verwendung von Vererbungsbeziehungen über die Selektoren mehrerer Knoten hinweg können gemeinsame Attribute wiederverwendet werden.

Das neue Graphmodell führt eine explizite Trennung zwischen Repräsentationen und Produkten ein.
Damit können Metadaten, die für ein gesamtes Produkt mit all seinen Repräsentationen gelten, an einem Ort gesammelt und ausgewertet werden.
Diese Metadaten können einfache Beschreibungen oder Referenzen sein, aber auch die Modellierung von Transformationen in den Produktknoten und Kanten zählt dazu.
Durch das Einführen von Transformationen konnte die Anzahl der notwendigen Knoten in gewissen Situationen drastisch reduziert werden.
Durch den Einsatz eines Graphens mit Knoten wird allgemein die Redundanz in den Ausprägungen der Repräsentationen reduziert, indem jede Repräsentation in nur einem Knoten modelliert wird, der durch Kanten beliebig oft in weiteren Knoten referenziert werden kann.
Mit der Typisierung der Knoten entsteht ein Modell, das leicht auf weitere Produktidentifikationsstandard-Ökosysteme erweitert werden kann.

Bei der Architektur des Graphen und der Implementierung in Java wurde an das Testframework gedacht, um eine frühzeitige Erkennung von Modellierungsfehlern im manuellen Prozess zu ermöglichen.

\subsection{Welche Herausforderungen bestehen weiterhin?}\label{subsec:discussion-negative}

Das Einführen eines graphbasiertn Modells bietet zwar deutlich bessere Möglichkeiten zur Sicherung der Stabilität und Integrität, bringt jedoch auch eine deutlich höhere konzeptionelle Komplexität mit sich.
Für interne, wie auch externe Beitragende bedeutet dies eine steilere Lernkurve im Vergleich zum bisherigen, flacheren Modellierungsansatz.
Gerade Fälle wie bei den verschachtelten Konstellationen des Windows-Referenzfalls (\autoref{subsec:example-windows}) zeigen, dass die Nachvollziehbarkeit über die Abhängigkeit der Knoten untereinander erschwert wird.
Auch wenn diese durch klare Regeln abgebildet werden und vom Konzept her gerade bei einer höheren Skalierung in der Modellierung einfacher zu handhaben sind, führt dies zu einem höheren Bedarf an Visualisierung und dokumentierter Herleitung.

Die Versionserkennung der Artefakte in den Transformationen erfolgt weiterhin über reguläre Ausdrücke.
Dies erlaubt zwar die Handhabung verschiedenster Versionierungsschemata, stellen jedoch in ihrer Erstellung und Pflege einen zusätzlichen Aufwand dar, der in zukünftigen Iterationen assistiert werden sollte.

Ein für die Praxis relevantes Element, das derzeit noch nicht umgesetzt ist, ist die Integration des neuen Systems in die bestehenden Correlation Utilities.
So fehlt etwa eine Visualisierungskomponente für den Graphen, mit der die Knoten und Beziehungen zwischen ihnen nachvollziehbar dargestellt werden kann.
Diese sollte durch Suchbegriffe Inseln im Graphen lokalisieren können, aber auch auf dem aktuell ausgewählten Artefakt alle ausgewerteten Pfade visualisieren.
Zudem wäre diese Visualisierung für die Entwicklung hilfreich sein, da der bisherige Ansatz den Graphen mit Graphviz\footnote{\url{https://graphviz.org}} zu rendern bei größeren Graphen nicht tragfähig ist.
Eine weitere Chance wäre es, eine Möglichkeit zu bieten, das Modifikationsformat nicht manuell in \acrshort{yaml} anlegen zu müssen, sondern auch dies durch ein Nutzerinterface zu ermöglichen.

Weiterhin ist das Testframework mit der automatischen Validierungskomponente noch nicht genügend ausspezifiziert und nicht implementiert.
Die Grundlagen für eine solche Prüfung sind gelegt, sie müssten jedoch in konkrete Werkzeuge umgesetzt werden.
Dieses soll den Graphen bei Modellierungs- und Aktualisierungsschritt auf innere Inkonsistenzen prüfen und als eine Grundlage für Regressionstests dienen.

Ein weiteres fehlendes Element ist ein JSON-Schema\footnote{\url{https://json-schema.org}} zur Validierung der manuellen Bearbeitung von \acrshort{yaml}-Modellierungsdateien.
Ein solches Schema ist im alten Korrelationssystem eine der einzigen Assistenzen, die Arbeitende mit den \acrshort{yaml}-Dateien erhalten und wird als sehr nützlich eingeschätzt.

Wie in \autoref{sec:evaluation-performanceanalyse} aufgeführt, ist das neue Korrelationssystem im Vergleich zum alten langsamer in der Laufzeit.
Da das System noch unoptimiert ist, sollte mit einem Profiler noch einige recht einfach zu findende Optimierungspotentiale offen sein.
